{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Reshape, Flatten, LSTM, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# functions for pre-processing texts \n",
    "# -----------------------------------\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    return(text)   \n",
    "\n",
    "# ----\n",
    "\n",
    "def read_train_data(file):\n",
    "        texts = [] \n",
    "        labels = []\n",
    "        df_train = pd.read_csv(file)  \n",
    "        line_num = 0\n",
    "        for idx in range(len(df_train)):\n",
    "            texts.append(preprocess(df_train['question_text'][idx]))\n",
    "            labels.append(df_train['target'][idx])\n",
    "            line_num += 1\n",
    "        return texts, labels\n",
    "    \n",
    "def read_test_data(file):\n",
    "        texts = [] \n",
    "        ids = []\n",
    "        df_test = pd.read_csv(file)\n",
    "        line_num = 0\n",
    "        for idx in range(len(df_test)):\n",
    "            texts.append(preprocess(df_test['question_text'][idx]))\n",
    "            ids.append(df_test['qid'][idx])\n",
    "            line_num += 1\n",
    "        return texts, ids\n",
    "\n",
    "# ---\n",
    "\n",
    "def preprocess_data(train_data_file, test_data_file, max_seq_len, split_ratio):\n",
    "\n",
    "        # 1) load train and test datasets\n",
    "        texts, labels= read_train_data(train_data_file)  \n",
    "        print('Finished loading train.csv: %s samples' % len(texts))\n",
    "        \n",
    "        test_texts, test_ids = read_test_data(test_data_file)\n",
    "        print('Finished loading test.csv: %s samples' % len(test_texts))\n",
    "                      \n",
    "        # 2) train the tokenizer\n",
    "        tokenizer = Tokenizer(num_words=200000)\n",
    "        tokenizer.fit_on_texts(texts + test_texts)        \n",
    "        word_index = tokenizer.word_index\n",
    "        print('%s tokens in total' % len(word_index))\n",
    "\n",
    "        # 3) sentences to sequences\n",
    "        train_sequences = tokenizer.texts_to_sequences(texts)\n",
    "        test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "        x = pad_sequences(train_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "        test_x = pad_sequences(test_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "    \n",
    "        # 4) final step\n",
    "        num_samples = len(x)\n",
    "        perm = np.random.permutation(num_samples)\n",
    "        idx = int(num_samples*split_ratio)\n",
    "        idx_train = perm[:idx]\n",
    "        idx_val = perm[idx:]\n",
    "        \n",
    "        train_x = x[idx_train]\n",
    "        val_x = x[idx_val]\n",
    "        \n",
    "        y = np.array(labels)\n",
    "        train_y = y[idx_train]\n",
    "        val_y = y[idx_val]\n",
    "            \n",
    "    \n",
    "        return train_x, train_y, val_x, val_y, test_x, test_ids, word_index\n",
    "\n",
    "# ------------------------------------------\n",
    "# function for building the model\n",
    "# ------------------------------------------\n",
    "\n",
    "def build_model(max_seq_len, word_index, embedding_dim, embedding_matrix):\n",
    "    \n",
    "    # 1) Embedding layer\n",
    "    inp = Input(shape=(max_seq_len,), dtype='int32')\n",
    "\n",
    "    x = Embedding(len(word_index)+1,\n",
    "                  embedding_dim,\n",
    "                  input_length=max_seq_len,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "\n",
    "    # 2) LSTM Layer\n",
    "    x = LSTM(64,dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 3) Dense Layer\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 4) Output Layer\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "78578eab64a477d0a5ad6b1c917ae154868a44df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading train.csv: 1306122 samples\n",
      "Finished loading test.csv: 375806 samples\n",
      "220853 tokens in total\n",
      "Shape of training data: (1044897, 30)\n",
      "Shape of training label: (1044897,)\n",
      "Shape of val data: (261225, 30)\n",
      "Shape of val label: (261225,)\n",
      "Shape of test data: (375806, 30)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1 Preprocessing texts (texts to numerical values)\n",
    "\n",
    "max_seq_len = 30\n",
    "split_ratio = 0.8\n",
    "train_file = '../input/quora-insincere-questions-classification/train.csv'\n",
    "test_file = '../input/quora-insincere-questions-classification/test.csv'\n",
    "train_x, train_y, val_x, val_y, test_x, test_ids, word_index = \\\n",
    "preprocess_data(train_file, test_file, max_seq_len, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:28, 10531.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2 Prepare embedding matrix\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max(list(word_index.values())) + 1, embedding_dim), dtype = 'float32')\n",
    "embedding_file='../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "f = open(embedding_file)\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    if word not in word_index:\n",
    "       continue\n",
    "    embedding_matrix[word_index[word]] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7a16003f33ff26675d1a9c5f4feafc5357c7b087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 30, 300)           66256200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 66,352,137\n",
      "Trainable params: 95,745\n",
      "Non-trainable params: 66,256,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3 Build and train model\n",
    "\n",
    "#keras.backend.clear_session()\n",
    "model = build_model(max_seq_len, word_index, embedding_dim, embedding_matrix)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/200\n",
      " - 26s - loss: 0.2872 - acc: 0.9151 - val_loss: 0.1295 - val_acc: 0.9522\n",
      "Epoch 2/200\n",
      " - 25s - loss: 0.1371 - acc: 0.9470 - val_loss: 0.1155 - val_acc: 0.9540\n",
      "Epoch 3/200\n",
      " - 25s - loss: 0.1239 - acc: 0.9503 - val_loss: 0.1112 - val_acc: 0.9557\n",
      "Epoch 4/200\n",
      " - 25s - loss: 0.1183 - acc: 0.9525 - val_loss: 0.1089 - val_acc: 0.9571\n",
      "Epoch 5/200\n",
      " - 25s - loss: 0.1150 - acc: 0.9541 - val_loss: 0.1068 - val_acc: 0.9580\n",
      "Epoch 6/200\n",
      " - 26s - loss: 0.1132 - acc: 0.9550 - val_loss: 0.1056 - val_acc: 0.9586\n",
      "Epoch 7/200\n",
      " - 25s - loss: 0.1108 - acc: 0.9562 - val_loss: 0.1046 - val_acc: 0.9588\n",
      "Epoch 8/200\n",
      " - 25s - loss: 0.1091 - acc: 0.9569 - val_loss: 0.1039 - val_acc: 0.9589\n",
      "Epoch 9/200\n",
      " - 25s - loss: 0.1079 - acc: 0.9570 - val_loss: 0.1026 - val_acc: 0.9595\n",
      "Epoch 10/200\n",
      " - 25s - loss: 0.1064 - acc: 0.9576 - val_loss: 0.1017 - val_acc: 0.9596\n",
      "Epoch 11/200\n",
      " - 25s - loss: 0.1054 - acc: 0.9580 - val_loss: 0.1012 - val_acc: 0.9594\n",
      "Epoch 12/200\n",
      " - 25s - loss: 0.1045 - acc: 0.9581 - val_loss: 0.1009 - val_acc: 0.9599\n",
      "Epoch 13/200\n",
      " - 25s - loss: 0.1034 - acc: 0.9588 - val_loss: 0.1002 - val_acc: 0.9600\n",
      "Epoch 14/200\n",
      " - 25s - loss: 0.1026 - acc: 0.9588 - val_loss: 0.0999 - val_acc: 0.9602\n",
      "Epoch 15/200\n",
      " - 25s - loss: 0.1018 - acc: 0.9593 - val_loss: 0.1000 - val_acc: 0.9600\n",
      "Epoch 16/200\n",
      " - 24s - loss: 0.1011 - acc: 0.9596 - val_loss: 0.0990 - val_acc: 0.9603\n",
      "Epoch 17/200\n",
      " - 25s - loss: 0.1006 - acc: 0.9597 - val_loss: 0.0987 - val_acc: 0.9605\n",
      "Epoch 18/200\n",
      " - 26s - loss: 0.0998 - acc: 0.9600 - val_loss: 0.0995 - val_acc: 0.9604\n",
      "Epoch 19/200\n",
      " - 24s - loss: 0.0993 - acc: 0.9602 - val_loss: 0.0984 - val_acc: 0.9605\n",
      "Epoch 20/200\n",
      " - 25s - loss: 0.0987 - acc: 0.9604 - val_loss: 0.0999 - val_acc: 0.9595\n",
      "Epoch 21/200\n",
      " - 24s - loss: 0.0985 - acc: 0.9605 - val_loss: 0.0990 - val_acc: 0.9607\n",
      "Epoch 22/200\n",
      " - 24s - loss: 0.0979 - acc: 0.9609 - val_loss: 0.0983 - val_acc: 0.9605\n",
      "Epoch 23/200\n",
      " - 25s - loss: 0.0977 - acc: 0.9608 - val_loss: 0.0984 - val_acc: 0.9600\n",
      "Epoch 24/200\n",
      " - 25s - loss: 0.0970 - acc: 0.9608 - val_loss: 0.0985 - val_acc: 0.9609\n",
      "Epoch 25/200\n",
      " - 25s - loss: 0.0966 - acc: 0.9610 - val_loss: 0.0984 - val_acc: 0.9609\n",
      "Epoch 26/200\n",
      " - 25s - loss: 0.0963 - acc: 0.9612 - val_loss: 0.0981 - val_acc: 0.9607\n",
      "Epoch 27/200\n",
      " - 26s - loss: 0.0958 - acc: 0.9614 - val_loss: 0.0980 - val_acc: 0.9604\n",
      "Epoch 28/200\n",
      " - 25s - loss: 0.0956 - acc: 0.9614 - val_loss: 0.0978 - val_acc: 0.9608\n",
      "Epoch 29/200\n",
      " - 26s - loss: 0.0950 - acc: 0.9616 - val_loss: 0.0984 - val_acc: 0.9609\n",
      "Epoch 30/200\n",
      " - 26s - loss: 0.0948 - acc: 0.9617 - val_loss: 0.0978 - val_acc: 0.9605\n",
      "Epoch 31/200\n",
      " - 25s - loss: 0.0947 - acc: 0.9616 - val_loss: 0.0987 - val_acc: 0.9608\n",
      "Epoch 32/200\n",
      " - 25s - loss: 0.0944 - acc: 0.9619 - val_loss: 0.0977 - val_acc: 0.9607\n",
      "Epoch 33/200\n",
      " - 26s - loss: 0.0940 - acc: 0.9617 - val_loss: 0.0979 - val_acc: 0.9603\n",
      "Epoch 34/200\n",
      " - 25s - loss: 0.0939 - acc: 0.9620 - val_loss: 0.0981 - val_acc: 0.9609\n",
      "Epoch 35/200\n",
      " - 25s - loss: 0.0936 - acc: 0.9621 - val_loss: 0.0989 - val_acc: 0.9604\n",
      "Epoch 36/200\n",
      " - 25s - loss: 0.0936 - acc: 0.9621 - val_loss: 0.0979 - val_acc: 0.9604\n",
      "Epoch 37/200\n",
      " - 24s - loss: 0.0931 - acc: 0.9622 - val_loss: 0.0985 - val_acc: 0.9609\n",
      "Min val loss is 0.09766278104929163\n"
     ]
    }
   ],
   "source": [
    "nb_epoches = 200\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_name = 'model_best.h5'\n",
    "model_checkpoint = ModelCheckpoint(model_name, save_best_only=True)\n",
    "\n",
    "hist = model.fit(train_x, train_y, \\\n",
    "                 validation_data=(val_x, val_y), \\\n",
    "                 epochs=nb_epoches, batch_size=2048, shuffle=True, verbose=2, \\\n",
    "                 callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(model_name)\n",
    "best_val_score = min(hist.history['val_loss']) \n",
    "print('Min val loss is', best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1b326f67d6f1ab028c37699cbb2acccc5b88dd53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 6s 17us/step\n"
     ]
    }
   ],
   "source": [
    "# --- Final step: submission\n",
    "\n",
    "preds = model.predict(test_x, batch_size=1024, verbose=1)\n",
    "preds = (preds > 0.35).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({'qid':test_ids, 'prediction':preds.ravel()})\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
